{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- --------\n",
      "appnope           0.1.2\n",
      "asttokens         2.0.5\n",
      "backcall          0.2.0\n",
      "debugpy           1.6.0\n",
      "decorator         5.1.1\n",
      "entrypoints       0.4\n",
      "executing         0.8.3\n",
      "ipykernel         6.11.0\n",
      "ipython           8.2.0\n",
      "jedi              0.18.1\n",
      "jupyter-client    7.2.1\n",
      "jupyter-core      4.9.2\n",
      "matplotlib-inline 0.1.3\n",
      "nest-asyncio      1.5.4\n",
      "numpy             1.22.3\n",
      "pandas            1.4.1\n",
      "parso             0.8.3\n",
      "pexpect           4.8.0\n",
      "pickleshare       0.7.5\n",
      "pip               22.0.4\n",
      "prompt-toolkit    3.0.28\n",
      "psutil            5.9.0\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "py4j              0.10.9.3\n",
      "Pygments          2.11.2\n",
      "pyspark           3.2.1\n",
      "python-dateutil   2.8.2\n",
      "pytz              2022.1\n",
      "pyzmq             22.3.0\n",
      "setuptools        61.3.0\n",
      "six               1.16.0\n",
      "stack-data        0.2.0\n",
      "tornado           6.1\n",
      "traitlets         5.1.1\n",
      "wcwidth           0.2.5\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The operation couldnâ€™t be completed. Unable to locate a Java Runtime.\n",
      "Please visit http://www.java.com for information on installing Java.\n",
      "\n",
      "/Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/bin/spark-class: line 96: CMD: bad array subscript\n",
      "head: illegal line count -- -1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sehatq/External/Repositories/adira_spark_cluster/test_cluster.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sehatq/External/Repositories/adira_spark_cluster/test_cluster.ipynb#ch0000002?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sehatq/External/Repositories/adira_spark_cluster/test_cluster.ipynb#ch0000002?line=1'>2</a>\u001b[0m       \u001b[39m.\u001b[39;49mmaster(\u001b[39m\"\u001b[39;49m\u001b[39mspark://7462adf4bdab:7077\u001b[39;49m\u001b[39m\"\u001b[39;49m) \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sehatq/External/Repositories/adira_spark_cluster/test_cluster.ipynb#ch0000002?line=2'>3</a>\u001b[0m       \u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mSparkEngineEx\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sehatq/External/Repositories/adira_spark_cluster/test_cluster.ipynb#ch0000002?line=3'>4</a>\u001b[0m       \u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[0;32m~/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py:228\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=225'>226</a>\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=226'>227</a>\u001b[0m     \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=227'>228</a>\u001b[0m     sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=228'>229</a>\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=229'>230</a>\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/sql/session.py?line=230'>231</a>\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc)\n",
      "File \u001b[0;32m~/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py:392\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=389'>390</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=390'>391</a>\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=391'>392</a>\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=392'>393</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=138'>139</a>\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=139'>140</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=143'>144</a>\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=144'>145</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=145'>146</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=146'>147</a>\u001b[0m                   conf, jsc, profiler_cls)\n",
      "File \u001b[0;32m~/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py:339\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=336'>337</a>\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=337'>338</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[0;32m--> <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=338'>339</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=339'>340</a>\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/context.py?line=341'>342</a>\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py?line=104'>105</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py?line=106'>107</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py?line=107'>108</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py?line=109'>110</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[1;32m    <a href='file:///Users/sehatq/External/Repositories/adira_spark_cluster/sandbox/lib/python3.8/site-packages/pyspark/java_gateway.py?line=110'>111</a>\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "      .master(\"spark://7462adf4bdab:7077\") \\\n",
    "      .appName(\"SparkEngineEx\")  \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbd3aa171faba6153c6be486736937328a7916f57fcb0ecbf21399aff9ddc9d1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 ('sandbox': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
